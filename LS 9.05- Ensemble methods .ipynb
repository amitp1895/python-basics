{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ensamble is collection of methods  , When the data set is huge in  volume then the module can not grasp and learn  the data . so  the data is devided into different into subsets and  given to different  models and the mean is  considered as the  model accuracy  . "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2 type of ensembel method \n",
    "1- Bagging - Parallael method -  where the subsets are given to diff models and  mean is considered as the  output accuracy\n",
    "sklearn->Random forest is the class for bagging \n",
    "\n",
    "2-boosithing - Sequential - here the data is learned by one model and the output is given to  another method for processing . it is caled the boosting technique . \n",
    "sklearn->adaboost , gradient boosting are the classes  for  boosting . \n",
    "\n",
    "there are other algorithm for sequential  boosting - xgboost ,catboost ,voting classifier  are the algo use for boosting but are not  part of sklearn .\n",
    "\n",
    "\n",
    "\n",
    "(Weak learner ) is a term used for individual model working in the  ensemble cluster , because  individual model  is not capable of learning the data set completely  ."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)[source]\n",
    "\n",
    "\n",
    " The n estimator  is the decision tree numbers - It denotes how many  \n",
    " default n estimator is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading iris dataset \n",
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()\n",
    "x=iris.data[:,:4]\n",
    "y=iris.target\n",
    "\n",
    "#splitting them \n",
    "x_train , x_test, y_train,y_test=train_test_split(x,y,test_size=0.28 , random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[17  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        17\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        43\n",
      "   macro avg       1.00      1.00      1.00        43\n",
      "weighted avg       1.00      1.00      1.00        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "rf=RandomForestClassifier (n_estimators=100 , random_state=42)\n",
    "#n_estimatorsint, default=100\n",
    "\n",
    "rf.fit(x_train,y_train)\n",
    "predrf=rf.predict(x_test)\n",
    "print(accuracy_score(y_test,predrf))\n",
    "print(confusion_matrix(y_test,predrf))\n",
    "print(classification_report(y_test,predrf))\n",
    "\n",
    "#n_estimator value can be changed to get bettern accuracy of the algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[17  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        17\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        43\n",
      "   macro avg       1.00      1.00      1.00        43\n",
      "weighted avg       1.00      1.00      1.00        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ad=AdaBoostClassifier()\n",
    "#n_estimatorsint, default=50\n",
    "\n",
    "ad.fit(x_train,y_train)\n",
    "predad=ad.predict(x_test)\n",
    "print(accuracy_score(y_test,predad))\n",
    "print(confusion_matrix(y_test,predad))\n",
    "print(classification_report(y_test,predad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[17  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        17\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        43\n",
      "   macro avg       1.00      1.00      1.00        43\n",
      "weighted avg       1.00      1.00      1.00        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ad=AdaBoostClassifier(n_estimators=100)\n",
    "#AdaBosstClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=50 ,Learning_rate=1.0)\n",
    "\n",
    "ad.fit(x_train,y_train)\n",
    "predad=ad.predict(x_test)\n",
    "print(accuracy_score(y_test,predad))\n",
    "print(confusion_matrix(y_test,predad))\n",
    "print(classification_report(y_test,predad))\n",
    "\n",
    "#in adaboost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NOTE ----- Bagging and  boosting can be done for both linear regression as well as  classification  . \n",
    "Random forest  has classifier and regressor \n",
    "adaboost has  classifier and regressor \n",
    "gradientboost  has classifier and regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NOTE -----In boosting we can change the base estimator  , but in bagging the base estimator is by default decision tree classifier . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5813953488372093\n",
      "[[ 0 17  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        17\n",
      "           1       0.41      0.92      0.57        13\n",
      "           2       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.58        43\n",
      "   macro avg       0.45      0.64      0.51        43\n",
      "weighted avg       0.41      0.58      0.46        43\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amit\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Amit\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Amit\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "svc=SVC() # here kernel value is by default rbf \n",
    "#create adaboost classifier object \n",
    "ada=AdaBoostClassifier(n_estimators=50,base_estimator=svc,algorithm='SAMME')\n",
    "ada.fit(x_train,y_train)\n",
    "ada_pred=ada.predict(x_test)\n",
    "\n",
    "print(accuracy_score(y_test,ada_pred))\n",
    "print(confusion_matrix(y_test,ada_pred))\n",
    "print(classification_report(y_test,ada_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9767441860465116\n",
      "[[17  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        17\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.98        43\n",
      "   macro avg       0.98      0.97      0.97        43\n",
      "weighted avg       0.98      0.98      0.98        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "svc=SVC(kernel='linear')\n",
    " #create adaboost classifier object \n",
    "ada=AdaBoostClassifier(n_estimators=50,base_estimator=svc,algorithm='SAMME')\n",
    "ada.fit(x_train,y_train)\n",
    "ada_pred=ada.predict(x_test)\n",
    "\n",
    "print(accuracy_score(y_test,ada_pred))\n",
    "print(confusion_matrix(y_test,ada_pred))\n",
    "print(classification_report(y_test,ada_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[17  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        17\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        43\n",
      "   macro avg       1.00      1.00      1.00        43\n",
      "weighted avg       1.00      1.00      1.00        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "gb=GradientBoostingClassifier()\n",
    "gb.fit(x_train ,y_train)\n",
    "predgb=gb.predict(x_test)\n",
    "\n",
    "print(accuracy_score(y_test,predgb))\n",
    "print(confusion_matrix(y_test,predgb))\n",
    "print(classification_report(y_test,predgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A voting  is a machine learning model that trains an ensemble models and predict an output (class) based on the voting proess\n",
    "\n",
    "Voting has 2 types of voting \n",
    "1-Hard voting -Works on final output voting \n",
    "2-Soft Voting - Works on avg of probaility for the prabale outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator=[]\n",
    "estimator.append(('LR',KNeighborsClassifier()))\n",
    "estimator.append(('SVC',SVC(gamma=\"auto\",probability=True)))\n",
    "estimator.append(('DTC',DecisionTreeClassifier()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LR', KNeighborsClassifier()),\n",
       " ('SVC', SVC(gamma='auto', probability=True)),\n",
       " ('DTC', DecisionTreeClassifier())]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1]\n",
      "Hard voting score  1\n"
     ]
    }
   ],
   "source": [
    "#voting clsiffier with hard voting\n",
    "vot_hard=VotingClassifier(estimators=estimator, voting='hard')\n",
    "vot_hard.fit(x_train,y_train)\n",
    "y_pred=vot_hard.predict(x_test)\n",
    "\n",
    "print(y_pred)\n",
    "#usnig accuracy score metric to predict accuracy \n",
    "score= accuracy_score(y_test, y_pred)\n",
    "print(\"Hard voting score % d\"% score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1]\n",
      "soft voting score  1\n"
     ]
    }
   ],
   "source": [
    "#voting clsiffier with soft voting\n",
    "vot_soft=VotingClassifier(estimators=estimator, voting='soft')\n",
    "vot_soft.fit(x_train,y_train)\n",
    "y_pred=vot_hard.predict(x_test)\n",
    "\n",
    "print(y_pred)\n",
    "#usnig accuracy score metric to predict accuracy \n",
    "score= accuracy_score(y_test, y_pred)\n",
    "print(\"soft voting score % d\"% score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
